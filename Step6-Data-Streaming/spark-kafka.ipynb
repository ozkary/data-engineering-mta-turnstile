{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Kafka Data Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Third-party library imports\n",
    "import pandas as pd\n",
    "import numpy\n",
    "import pyspark\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import udf, col, from_json, from_csv, sum as _sum\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType, DateType\n",
    "\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.1,org.apache.spark:spark-avro_2.12:3.3.1 pyspark-shell'\n",
    "\n",
    "print(\"pyspark \",pyspark.__version__)\n",
    "print(\"pandas \", pd.__version__)\n",
    "print(\"numpy \", numpy.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Configuration file reader\n",
    "\n",
    "def read_config(config_file):\n",
    "    \"\"\"\n",
    "    Reads the kafka configuration information that is stored in the system    \n",
    "    \"\"\"\n",
    "    conf = {}    \n",
    "    with open(config_file) as fh:\n",
    "        for line in fh:\n",
    "            line = line.strip()\n",
    "            if len(line) != 0 and line[0] != \"#\":\n",
    "                parameter, value = line.strip().split('=', 1)\n",
    "                conf[parameter] = value.strip()          \n",
    "    return conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the local configuration files\n",
    "\n",
    "config_path = os.path.join(os.path.dirname('/home/ozkary/.kafka/'),'localhost-nosasl.properties')\n",
    "config = read_config(config_path)\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, DataFrame\n",
    "import pyspark.sql.types as T\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Spark-Notebook\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read from Kafka Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "topic = 'mta-turnstile'\n",
    "client_id = 'Spark-Notebook-Session'\n",
    "group_id = 'turnstile'\n",
    "\n",
    "use_sasl = \"sasl.mechanism\" in config and config[\"sasl.mechanism\"] is not None\n",
    "\n",
    "kafka_options = {\n",
    "            \"kafka.bootstrap.servers\": config[\"bootstrap.servers\"],\n",
    "            \"subscribe\": topic,\n",
    "            \"startingOffsets\": \"latest\",\n",
    "            \"failOnDataLoss\": \"false\",\n",
    "            \"client.id\": client_id,            \n",
    "            \"group.id\": group_id,            \n",
    "            \"auto.offset.reset\": \"latest\",\n",
    "            \"checkpointLocation\": \"checkpoint\",\n",
    "            \"minPartitions\": \"2\",\n",
    "            \"enable.auto.commit\": \"false\",\n",
    "            \"enable.partition.eof\": \"true\"                        \n",
    "        }          \n",
    "\n",
    "if use_sasl:\n",
    "    # set the JAAS configuration only when use_sasl is True\n",
    "    sasl_config = f'org.apache.kafka.common.security.plain.PlainLoginModule required serviceName=\"kafka\" username=\"{settings[\"sasl.username\"]}\" password=\"{settings[\"sasl.password\"]}\";'\n",
    "\n",
    "    login_options = {\n",
    "        \"kafka.sasl.mechanisms\": settings[\"sasl.mechanism\"],\n",
    "        \"kafka.security.protocol\": settings[\"security.protocol\"],\n",
    "        \"kafka.sasl.username\": settings[\"sasl.username\"],\n",
    "        \"kafka.sasl.password\": settings[\"sasl.password\"],  \n",
    "        \"kafka.sasl.jaas.config\": sasl_config          \n",
    "    }\n",
    "    # merge the login options with the kafka options\n",
    "    kafka_options = {**kafka_options, **login_options}  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_deserializer(value: bytes) -> any:\n",
    "    \"\"\"\n",
    "    Message value deserializer\n",
    "    \"\"\"\n",
    "    return json.loads(value) \n",
    "\n",
    "# set the stream source\n",
    "# default for startingOffsets is \"latest\"\n",
    "stream = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .options(**kafka_options) \\\n",
    "    .option(\"key.deserializer\", value_deserializer) \\\n",
    "    .option(\"value.deserializer\", value_deserializer) \\\n",
    "    .load()\n",
    "\n",
    "\n",
    "stream.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current timestamp\n",
    "current_timestamp = F.current_timestamp()\n",
    "print(current_timestamp[0])\n",
    "\n",
    "timestamp = datetime.now()\n",
    "print(timestamp)\n",
    "\n",
    "# Format the timestamp as needed\n",
    "time = timestamp.strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "print(time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def write_to_console(df: DataFrame, output_mode: str = 'append', processing_time: str = '60 seconds') -> None:\n",
    "    \"\"\"\n",
    "        Output stream values to the console\n",
    "    \"\"\"\n",
    "    \n",
    "    console_query = df.writeStream\\\n",
    "        .outputMode(output_mode) \\\n",
    "        .trigger(processingTime=processing_time) \\\n",
    "        .format(\"console\") \\\n",
    "        .option(\"truncate\", False) \\\n",
    "        .start()\n",
    "    \n",
    "    # console_query.awaitTermination()   \n",
    "\n",
    "def process_mini_batch(df, batch_id, path):\n",
    "    \"\"\"Processes a mini-batch, converts to Pandas, and writes to GCP Storage as CSV.gz.\"\"\"\n",
    "\n",
    "     # Check if DataFrame is empty\n",
    "    if df.count() == 0:\n",
    "        print(f\"DataFrame for batch {batch_id} is empty. Skipping processing.\")\n",
    "        return\n",
    "\n",
    "    # Convert to Pandas DataFrame\n",
    "    df_pandas = df.toPandas()\n",
    "\n",
    "    print(df_pandas.head())\n",
    "\n",
    "    # Get the current timestamp\n",
    "    timestamp = datetime.now()\n",
    "    # Format the timestamp as needed\n",
    "    time = timestamp.strftime(\"%Y%m%d_%H%M%S\")    \n",
    "\n",
    "    # Write to GCP Storage as CSV.gz\n",
    "    # with fsspec.open(f\"gs://your-bucket/path/batch_{batch_id}.csv.gz\", \"wb\") as f:\n",
    "    file_path = f\"{path}/batch_{batch_id}_{time}.csv\"\n",
    "    df_pandas.to_csv(file_path, index=False)\n",
    "    # , compression=\"gzip\")\n",
    "\n",
    "\n",
    "# write a streaming data frame to storage ./storage\n",
    "def write_to_storage(df: DataFrame, output_mode: str = 'append', processing_time: str = '60 seconds') -> None:\n",
    "    \"\"\"\n",
    "        Output stream values to the console\n",
    "    \"\"\"   \n",
    "    df_csv = df.select(\n",
    "        \"CA\", \"UNIT\", \"SCP\", \"STATION\", \"LINENAME\", \"DIVISION\", \"DATE\", \"DESC\", \"TIME\",\"ENTRIES\", \"EXITS\"\n",
    "    )\n",
    "\n",
    "    # time_value = df.select(\"START\").first()[\"START\"]\n",
    "    # path = f\"./storage/window_{F.date_format(F.to_timestamp(time_value), 'yyyyMMdd_HHmmss')}\"\n",
    "        \n",
    "    path = \"./storage/\"     \n",
    "    storage_query = df_csv.writeStream \\\n",
    "        .outputMode(output_mode) \\\n",
    "        .trigger(processingTime=processing_time) \\\n",
    "        .format(\"csv\") \\\n",
    "        .option(\"header\", True) \\\n",
    "        .option(\"path\", path) \\\n",
    "        .option(\"checkpointLocation\", \"./checkpoint\") \\\n",
    "        .foreachBatch(lambda batch, id: process_mini_batch(batch, id, path)) \\\n",
    "        .option(\"truncate\", False) \\\n",
    "        .start()\n",
    "    \n",
    "    # .option(\"compression\", \"gzip\") \\\n",
    "\n",
    "    # try:\n",
    "    #     # Wait for the streaming query to terminate\n",
    "    #     storage_query.awaitTermination()\n",
    "    # except KeyboardInterrupt:\n",
    "    #     # Handle keyboard interrupt (e.g., Ctrl+C)\n",
    "    #     storage_query.stop()\n",
    "\n",
    "# Define the schema for the incoming data\n",
    "turnstiles_schema = StructType([\n",
    "    StructField(\"CA\", StringType()),\n",
    "    StructField(\"UNIT\", StringType()),\n",
    "    StructField(\"SCP\", StringType()),\n",
    "    StructField(\"STATION\", StringType()),\n",
    "    StructField(\"LINENAME\", StringType()),\n",
    "    StructField(\"DIVISION\", StringType()),\n",
    "    StructField(\"DATE\", StringType()),\n",
    "    StructField(\"TIME\", StringType()),\n",
    "    StructField(\"DESC\", StringType()),\n",
    "    StructField(\"ENTRIES\", IntegerType()),\n",
    "    StructField(\"EXITS\", IntegerType()),\n",
    "    StructField(\"ID\", StringType()),\n",
    "    StructField(\"TIMESTAMP\", StringType())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parse_messages(stream, schema) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Parse the messages and use the provided schema to type cast the fields\n",
    "    \"\"\"\n",
    "    assert stream.isStreaming is True, \"DataFrame doesn't receive streaming data\"\n",
    "\n",
    "    options =  {'header': 'true', 'sep': ','}\n",
    "    df = stream.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\", \"timestamp\")               \n",
    "                                \n",
    "    # print(\"df =====>\",df)\n",
    "    # split attributes to nested array in one Column\n",
    "    col = F.split(df['value'], ',')\n",
    "    \n",
    "    # expand col to multiple top-level columns\n",
    "    for idx, field in enumerate(schema):\n",
    "        df = df.withColumn(field.name, col.getItem(idx).cast(field.dataType))\n",
    "        \n",
    "    # remove quotes from TIMESTAMP column\n",
    "    df = df.withColumn(\"TIMESTAMP\", F.regexp_replace(F.col(\"TIMESTAMP\"), '\"', ''))    \n",
    "    df = df.withColumn(\"CA\", F.regexp_replace(F.col(\"CA\"), '\"', ''))    \n",
    "    \n",
    "    result = df.select([field.name for field in schema])    \n",
    "\n",
    "    df.dropDuplicates([\"ID\",\"STATION\",\"TIMESTAMP\"])\n",
    "\n",
    "    result.printSchema()\n",
    "    \n",
    "    return result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agg_messages(df, window_duration: str, window_slide: str) -> DataFrame:\n",
    "        \"\"\"\n",
    "            Window for n minutes aggregations group by AC, UNIT, STATION, DATE, DESC\n",
    "        \"\"\"\n",
    "        \n",
    "        # Ensure TIMESTAMP is in the correct format (timestamp type)    \n",
    "        date_format = \"yyyy-MM-dd HH:mm:ss\"        \n",
    "        df = df.withColumn(\"TS\", F.to_timestamp(\"TIMESTAMP\", date_format))    \n",
    "\n",
    "        df_windowed = df \\\n",
    "            .withWatermark(\"TS\", window_duration) \\\n",
    "            .groupBy(F.window(\"TS\", window_duration, window_slide),\"CA\", \"UNIT\",\"SCP\",\"STATION\",\"LINENAME\",\"DIVISION\", \"DATE\", \"DESC\") \\\n",
    "            .agg(\n",
    "                F.sum(\"ENTRIES\").alias(\"ENTRIES\"),\n",
    "                F.sum(\"EXITS\").alias(\"EXITS\")\n",
    "            ).withColumn(\"START\",F.col(\"window.start\")) \\\n",
    "            .withColumn(\"END\", F.col(\"window.end\")) \\\n",
    "            .withColumn(\"TIME\", F.date_format(\"window.end\", \"HH:mm:ss\")) \\\n",
    "            .drop(\"window\") \\\n",
    "            .select(\"CA\",\"UNIT\",\"SCP\",\"STATION\",\"LINENAME\",\"DIVISION\",\"DATE\",\"DESC\",\"TIME\",\"START\",\"END\",\"ENTRIES\",\"EXITS\")\n",
    "        \n",
    "        df_windowed.printSchema()            \n",
    "\n",
    "        return df_windowed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_by_station(df, window_duration: str, window_slide: str) -> DataFrame:\n",
    "    \n",
    "    # Ensure TIMESTAMP is in the correct format (timestamp type)    \n",
    "    date_format = \"yyyy-MM-dd HH:mm:ss\"        \n",
    "    df = df.withColumn(\"TS\", F.to_timestamp(\"TIMESTAMP\", date_format))    \n",
    "\n",
    "    df_windowed = df \\\n",
    "        .withWatermark(\"TS\", window_duration) \\\n",
    "        .groupBy(F.window(\"TS\", window_duration, window_slide), \"STATION\") \\\n",
    "        .agg(\n",
    "            F.sum(\"ENTRIES\").alias(\"ENTRIES\"),\n",
    "            F.sum(\"EXITS\").alias(\"EXITS\")\n",
    "        ).withColumn(\"START\",F.col(\"window.start\")) \\\n",
    "        .withColumn(\"END\", F.col(\"window.end\")) \\\n",
    "        .withColumn(\"TIME\", F.date_format(\"window.start\", \"HH:mm:ss\")) \\\n",
    "        .drop(\"window\") \\\n",
    "        .select(\"STATION\",\"TIME\",\"START\",\"END\",\"ENTRIES\",\"EXITS\")\n",
    "    \n",
    "    df_windowed.printSchema()\n",
    "    return df_windowed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch(df, id, tag='message'):\n",
    "\n",
    "    # get the values from the first row\n",
    "    row = df.first()\n",
    "    # check if the TIMESTAMP value can be casted as timestamp\n",
    "    # if not, the row is invalid and we can skip the batch\n",
    "\n",
    "    # if row is None:\n",
    "    #     # print(f\"Invalid {tag} batch {id}\")\n",
    "    #     return\n",
    "    \n",
    "    # ts = row['TIMESTAMP']\n",
    "\n",
    "    # try:\n",
    "    #     row['TIMESTAMP'].cast(\"timestamp\")\n",
    "    # except:\n",
    "    #     print(f\"Invalid TIMESTAMP {ts} value in batch {id}\")\n",
    "    \n",
    "    print(f\"Processing {tag} batch {id} with {df.count()} records. {row}\")\n",
    "    # if df.isEmpty():\n",
    "    #     print(f\"DataFrame is empty in this batch {id}.\")\n",
    "    #     # Handle empty DataFrame as needed\n",
    "    # else:\n",
    "    #      print(\"Data found in this batch.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- AC: string (nullable = true)\n",
      " |-- UNIT: string (nullable = true)\n",
      " |-- SCP: string (nullable = true)\n",
      " |-- STATION: string (nullable = true)\n",
      " |-- LINENAME: string (nullable = true)\n",
      " |-- DIVISION: string (nullable = true)\n",
      " |-- DATE: string (nullable = true)\n",
      " |-- TIME: string (nullable = true)\n",
      " |-- DESC: string (nullable = true)\n",
      " |-- ENTRIES: integer (nullable = true)\n",
      " |-- EXITS: integer (nullable = true)\n",
      " |-- ID: string (nullable = true)\n",
      " |-- TIMESTAMP: string (nullable = true)\n",
      "\n",
      "24/01/29 14:09:39 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-cd47533e-7061-46bf-ab06-35ead7996814. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "24/01/29 14:09:39 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "root\n",
      " |-- AC: string (nullable = true)\n",
      " |-- UNIT: string (nullable = true)\n",
      " |-- SCP: string (nullable = true)\n",
      " |-- STATION: string (nullable = true)\n",
      " |-- LINENAME: string (nullable = true)\n",
      " |-- DIVISION: string (nullable = true)\n",
      " |-- DATE: string (nullable = true)\n",
      " |-- DESC: string (nullable = true)\n",
      " |-- TIME: string (nullable = true)\n",
      " |-- START: timestamp (nullable = true)\n",
      " |-- END: timestamp (nullable = true)\n",
      " |-- ENTRIES: long (nullable = true)\n",
      " |-- EXITS: long (nullable = true)\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "24/01/29 14:09:39 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "+---+----+---+-------+--------+--------+----+----+----+-------+-----+---+---------+\n",
      "|AC |UNIT|SCP|STATION|LINENAME|DIVISION|DATE|TIME|DESC|ENTRIES|EXITS|ID |TIMESTAMP|\n",
      "+---+----+---+-------+--------+--------+----+----+----+-------+-----+---+---------+\n",
      "+---+----+---+-------+--------+--------+----+----+----+-------+-----+---+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 67:>                                                         (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/01/29 14:09:40 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:40 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:40 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:40 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:40 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:40 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:40 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:40 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:41 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:41 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:41 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:41 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:41 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:41 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:41 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:41 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:41 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:41 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:41 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:41 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:41 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:41 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:41 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:41 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:41 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:41 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:41 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:41 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:41 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:41 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:41 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:41 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:41 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:41 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:41 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:41 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:41 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:41 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:41 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 68:========>                                              (32 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/01/29 14:09:41 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:41 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:41 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:41 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:41 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:41 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:41 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:41 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:41 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:41 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:41 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:41 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:41 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:41 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:41 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:41 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:41 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:41 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:41 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:41 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:41 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:41 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:41 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:41 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:41 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:41 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 68:===============>                                       (57 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/01/29 14:09:41 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:41 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:41 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:41 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:41 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:41 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:41 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:42 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:42 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:42 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:42 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:42 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:42 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:42 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 68:======================>                                (81 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/01/29 14:09:42 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:42 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:42 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:42 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:42 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:42 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:42 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:42 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:42 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:42 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:42 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:42 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:42 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:42 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:42 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:42 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:42 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:42 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:42 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:42 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:42 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:42 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:42 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:42 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 68:===========================>                          (103 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/01/29 14:09:42 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:42 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:42 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:42 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:42 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:42 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:42 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:42 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:42 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:42 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:42 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:42 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:42 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:42 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:42 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:42 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:42 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:42 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:42 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:42 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:42 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:42 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:42 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 68:==================================>                   (127 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/01/29 14:09:42 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:43 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:43 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:43 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:43 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:43 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:43 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:43 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:43 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:43 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:43 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:43 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:43 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:43 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:43 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:43 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:43 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:43 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:43 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:43 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:43 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:43 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:43 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:43 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:43 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 68:========================================>             (149 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/01/29 14:09:43 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:43 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:43 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:43 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:43 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:43 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:43 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:43 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:43 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:43 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:43 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:43 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:43 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:43 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:43 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:43 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:43 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:43 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:43 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:43 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:43 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:43 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:43 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:43 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:43 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:43 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:43 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:43 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 68:==============================================>       (172 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/01/29 14:09:43 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:43 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:43 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:43 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:43 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:43 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:43 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:44 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:44 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 68:====================================================> (194 + 6) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/01/29 14:09:44 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:44 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:44 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:44 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:44 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:44 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:44 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:44 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:44 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:44 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:44 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "24/01/29 14:09:44 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     AC  UNIT       SCP        STATION LINENAME DIVISION      DATE     DESC  \\\n",
      "0  A001  R002  02-00-00  Test-Station1   456NQR      BMT  01-29-24  REGULAR   \n",
      "1  A002  R002  02-00-00  Test-Station2   456NQR      BMT  01-29-24  REGULAR   \n",
      "2  A001  R001  02-00-00  Test-Station2   456NQR      BMT  01-29-24  REGULAR   \n",
      "3  A001  R001  02-00-00  Test-Station1   456NQR      BMT  01-29-24  REGULAR   \n",
      "4  A002  R001  02-00-00  Test-Station1   456NQR      BMT  01-29-24  REGULAR   \n",
      "\n",
      "       TIME  ENTRIES  EXITS  \n",
      "0  13:55:00       32     27  \n",
      "1  13:55:00       33     29  \n",
      "2  13:55:00       19     19  \n",
      "3  13:55:00       28     34  \n",
      "4  13:55:00       48     44  \n",
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+----+----+--------+-------------+--------+--------+--------+--------+-------+-------+-----+------------------------------------+-------------------+\n",
      "|AC  |UNIT|SCP     |STATION      |LINENAME|DIVISION|DATE    |TIME    |DESC   |ENTRIES|EXITS|ID                                  |TIMESTAMP          |\n",
      "+----+----+--------+-------------+--------+--------+--------+--------+-------+-------+-----+------------------------------------+-------------------+\n",
      "|A001|R002|02-00-00|Test-Station1|456NQR  |BMT     |01-29-24|14:09:41|REGULAR|6      |6    |142e407c-4a15-4014-aa84-4c3eef10f63e|2024-01-29 14:09:41|\n",
      "|A002|R001|02-00-00|Test-Station2|456NQR  |BMT     |01-29-24|14:09:51|REGULAR|8      |9    |2bee4b4d-cd22-496c-a497-e27986f67661|2024-01-29 14:09:51|\n",
      "+----+----+--------+-------------+--------+--------+--------+--------+-------+-------+-----+------------------------------------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     AC  UNIT       SCP        STATION LINENAME DIVISION      DATE     DESC  \\\n",
      "0  A002  R001  02-00-00  Test-Station1   456NQR      BMT  01-29-24  REGULAR   \n",
      "1  A001  R001  02-00-00  Test-Station2   456NQR      BMT  01-29-24  REGULAR   \n",
      "2  A002  R001  02-00-00  Test-Station2   456NQR      BMT  01-29-24  REGULAR   \n",
      "3  A001  R002  02-00-00  Test-Station2   456NQR      BMT  01-29-24  REGULAR   \n",
      "4  A001  R001  02-00-00  Test-Station1   456NQR      BMT  01-29-24  REGULAR   \n",
      "\n",
      "       TIME  ENTRIES  EXITS  \n",
      "0  14:00:00       22     19  \n",
      "1  14:00:00       27     33  \n",
      "2  14:00:00       13     15  \n",
      "3  14:00:00       24     25  \n",
      "4  14:00:00       31     25  \n",
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+----+----+--------+-------------+--------+--------+--------+--------+-------+-------+-----+------------------------------------+-------------------+\n",
      "|AC  |UNIT|SCP     |STATION      |LINENAME|DIVISION|DATE    |TIME    |DESC   |ENTRIES|EXITS|ID                                  |TIMESTAMP          |\n",
      "+----+----+--------+-------------+--------+--------+--------+--------+-------+-------+-----+------------------------------------+-------------------+\n",
      "|A001|R002|02-00-00|Test-Station2|456NQR  |BMT     |01-29-24|14:10:01|REGULAR|8      |8    |406d88a4-708e-41c0-a473-b9f604836bb6|2024-01-29 14:10:01|\n",
      "|A001|R002|02-00-00|Test-Station1|456NQR  |BMT     |01-29-24|14:10:11|REGULAR|10     |8    |0cb34556-9c6f-41bf-a43b-97671e9b1304|2024-01-29 14:10:11|\n",
      "|A002|R001|02-00-00|Test-Station2|456NQR  |BMT     |01-29-24|14:10:21|REGULAR|5      |6    |5af42e73-ce81-4812-831b-ddb3235b49d3|2024-01-29 14:10:21|\n",
      "|A002|R002|02-00-00|Test-Station1|456NQR  |BMT     |01-29-24|14:10:31|REGULAR|7      |9    |c02374b5-8017-4c1a-ba6e-3de902ee2369|2024-01-29 14:10:31|\n",
      "|A002|R002|02-00-00|Test-Station1|456NQR  |BMT     |01-29-24|14:10:41|REGULAR|10     |10   |6b5698b1-5d32-46b0-9669-f543c6d7c7e4|2024-01-29 14:10:41|\n",
      "|A002|R002|02-00-00|Test-Station1|456NQR  |BMT     |01-29-24|14:10:51|REGULAR|5      |6    |6b0200d8-208a-4a2c-b79a-a09405901b4c|2024-01-29 14:10:51|\n",
      "+----+----+--------+-------------+--------+--------+--------+--------+-------+-------+-----+------------------------------------+-------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 3\n",
      "-------------------------------------------\n",
      "+----+----+--------+-------------+--------+--------+--------+--------+-------+-------+-----+------------------------------------+-------------------+\n",
      "|AC  |UNIT|SCP     |STATION      |LINENAME|DIVISION|DATE    |TIME    |DESC   |ENTRIES|EXITS|ID                                  |TIMESTAMP          |\n",
      "+----+----+--------+-------------+--------+--------+--------+--------+-------+-------+-----+------------------------------------+-------------------+\n",
      "|A002|R001|02-00-00|Test-Station2|456NQR  |BMT     |01-29-24|14:11:01|REGULAR|6      |10   |2714d946-0a77-4159-8138-f6f258179387|2024-01-29 14:11:01|\n",
      "|A002|R002|02-00-00|Test-Station1|456NQR  |BMT     |01-29-24|14:11:11|REGULAR|10     |8    |d14b653d-9d0e-4ec2-afe4-b6bb575dbe0f|2024-01-29 14:11:11|\n",
      "|A002|R001|02-00-00|Test-Station1|456NQR  |BMT     |01-29-24|14:11:21|REGULAR|10     |5    |aee0404c-70a8-43a4-8c18-862af000ed96|2024-01-29 14:11:21|\n",
      "|A001|R002|02-00-00|Test-Station1|456NQR  |BMT     |01-29-24|14:11:31|REGULAR|8      |6    |dabc1b51-0301-4299-beca-c6805f19624b|2024-01-29 14:11:31|\n",
      "|A001|R002|02-00-00|Test-Station2|456NQR  |BMT     |01-29-24|14:11:41|REGULAR|7      |9    |64573a4b-c781-4475-ad11-f3a79399f24a|2024-01-29 14:11:41|\n",
      "|A001|R001|02-00-00|Test-Station2|456NQR  |BMT     |01-29-24|14:11:51|REGULAR|10     |10   |6ba3fe32-ab19-402a-a6f3-eec2b1a702db|2024-01-29 14:11:51|\n",
      "+----+----+--------+-------------+--------+--------+--------+--------+-------+-------+-----+------------------------------------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 4\n",
      "-------------------------------------------\n",
      "+----+----+--------+-------------+--------+--------+--------+--------+-------+-------+-----+------------------------------------+-------------------+\n",
      "|AC  |UNIT|SCP     |STATION      |LINENAME|DIVISION|DATE    |TIME    |DESC   |ENTRIES|EXITS|ID                                  |TIMESTAMP          |\n",
      "+----+----+--------+-------------+--------+--------+--------+--------+-------+-------+-----+------------------------------------+-------------------+\n",
      "|A001|R002|02-00-00|Test-Station2|456NQR  |BMT     |01-29-24|14:12:01|REGULAR|8      |8    |a2a2e18f-9c59-4951-a372-53802d5105a6|2024-01-29 14:12:01|\n",
      "|A001|R001|02-00-00|Test-Station1|456NQR  |BMT     |01-29-24|14:12:11|REGULAR|8      |9    |9858d675-edd5-4b06-8bfb-a96d5f6a4b7f|2024-01-29 14:12:11|\n",
      "|A002|R001|02-00-00|Test-Station1|456NQR  |BMT     |01-29-24|14:12:21|REGULAR|10     |9    |a2dd934d-9e4e-4d31-9ba0-c50d764c4faa|2024-01-29 14:12:21|\n",
      "|A001|R001|02-00-00|Test-Station2|456NQR  |BMT     |01-29-24|14:12:31|REGULAR|5      |6    |56672142-b198-4098-a3c8-f82970e81734|2024-01-29 14:12:31|\n",
      "|A002|R002|02-00-00|Test-Station1|456NQR  |BMT     |01-29-24|14:12:41|REGULAR|9      |10   |1f54c333-4004-431b-a639-a556d30343f3|2024-01-29 14:12:41|\n",
      "|A002|R002|02-00-00|Test-Station2|456NQR  |BMT     |01-29-24|14:12:51|REGULAR|10     |8    |a2c0449d-c608-4d20-a7cf-678c4e70d4a9|2024-01-29 14:12:51|\n",
      "+----+----+--------+-------------+--------+--------+--------+--------+-------+-------+-----+------------------------------------+-------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 5\n",
      "-------------------------------------------\n",
      "+----+----+--------+-------------+--------+--------+--------+--------+-------+-------+-----+------------------------------------+-------------------+\n",
      "|AC  |UNIT|SCP     |STATION      |LINENAME|DIVISION|DATE    |TIME    |DESC   |ENTRIES|EXITS|ID                                  |TIMESTAMP          |\n",
      "+----+----+--------+-------------+--------+--------+--------+--------+-------+-------+-----+------------------------------------+-------------------+\n",
      "|A001|R002|02-00-00|Test-Station2|456NQR  |BMT     |01-29-24|14:13:01|REGULAR|6      |9    |4a03e55b-4907-4842-86de-f8787fc6be58|2024-01-29 14:13:01|\n",
      "|A002|R002|02-00-00|Test-Station2|456NQR  |BMT     |01-29-24|14:13:11|REGULAR|9      |9    |342e02f4-80fb-4617-92b7-9493d1b9e69b|2024-01-29 14:13:11|\n",
      "|A001|R002|02-00-00|Test-Station1|456NQR  |BMT     |01-29-24|14:13:21|REGULAR|6      |7    |b9ecf68e-9828-4d55-ae7e-56616644c92b|2024-01-29 14:13:21|\n",
      "|A001|R001|02-00-00|Test-Station2|456NQR  |BMT     |01-29-24|14:13:31|REGULAR|6      |7    |2e569e54-b27d-4a71-be2e-0c6e14dd6f1e|2024-01-29 14:13:31|\n",
      "|A001|R002|02-00-00|Test-Station1|456NQR  |BMT     |01-29-24|14:13:41|REGULAR|5      |9    |5abe719a-1700-4b3b-a619-d71c5306cb91|2024-01-29 14:13:41|\n",
      "|A002|R002|02-00-00|Test-Station1|456NQR  |BMT     |01-29-24|14:13:51|REGULAR|7      |10   |de4e7405-c200-4407-9c02-3c06abe2b735|2024-01-29 14:13:51|\n",
      "+----+----+--------+-------------+--------+--------+--------+--------+-------+-------+-----+------------------------------------+-------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 6\n",
      "-------------------------------------------\n",
      "+----+----+--------+-------------+--------+--------+--------+--------+-------+-------+-----+------------------------------------+-------------------+\n",
      "|AC  |UNIT|SCP     |STATION      |LINENAME|DIVISION|DATE    |TIME    |DESC   |ENTRIES|EXITS|ID                                  |TIMESTAMP          |\n",
      "+----+----+--------+-------------+--------+--------+--------+--------+-------+-------+-----+------------------------------------+-------------------+\n",
      "|A001|R002|02-00-00|Test-Station1|456NQR  |BMT     |01-29-24|14:14:02|REGULAR|10     |9    |d0b207b3-2570-47cd-96f4-5498b87823c5|2024-01-29 14:14:02|\n",
      "|A001|R001|02-00-00|Test-Station1|456NQR  |BMT     |01-29-24|14:14:12|REGULAR|6      |6    |16bf46ef-70c9-41e0-8419-81774acb1327|2024-01-29 14:14:12|\n",
      "|A001|R001|02-00-00|Test-Station2|456NQR  |BMT     |01-29-24|14:14:22|REGULAR|6      |7    |c9b95768-ec5d-41b0-9ecb-d46a1e58d7f5|2024-01-29 14:14:22|\n",
      "|A002|R001|02-00-00|Test-Station1|456NQR  |BMT     |01-29-24|14:14:32|REGULAR|8      |10   |787053e5-c44a-493d-bf6e-f386cc078e03|2024-01-29 14:14:32|\n",
      "|A002|R002|02-00-00|Test-Station1|456NQR  |BMT     |01-29-24|14:14:42|REGULAR|6      |10   |314d5573-7e16-4437-bab9-e8809aef5446|2024-01-29 14:14:42|\n",
      "|A002|R001|02-00-00|Test-Station1|456NQR  |BMT     |01-29-24|14:14:52|REGULAR|8      |5    |a7310c5c-98f9-45f3-afaf-43578fcfff07|2024-01-29 14:14:52|\n",
      "+----+----+--------+-------------+--------+--------+--------+--------+-------+-------+-----+------------------------------------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame for batch 11 is empty. Skipping processing.\n",
      "-------------------------------------------\n",
      "Batch: 7\n",
      "-------------------------------------------\n",
      "+----+----+--------+-------------+--------+--------+--------+--------+-------+-------+-----+------------------------------------+-------------------+\n",
      "|AC  |UNIT|SCP     |STATION      |LINENAME|DIVISION|DATE    |TIME    |DESC   |ENTRIES|EXITS|ID                                  |TIMESTAMP          |\n",
      "+----+----+--------+-------------+--------+--------+--------+--------+-------+-------+-----+------------------------------------+-------------------+\n",
      "|A002|R002|02-00-00|Test-Station2|456NQR  |BMT     |01-29-24|14:15:02|REGULAR|5      |6    |6fad3a0e-8cb3-4354-a6b6-a5dccf649b48|2024-01-29 14:15:02|\n",
      "|A002|R002|02-00-00|Test-Station1|456NQR  |BMT     |01-29-24|14:15:12|REGULAR|5      |9    |e139af12-cdc5-412f-ab9e-053b915eebcc|2024-01-29 14:15:12|\n",
      "|A001|R002|02-00-00|Test-Station2|456NQR  |BMT     |01-29-24|14:15:22|REGULAR|5      |9    |3bf4c3d9-d08d-42e3-a9e6-7bc4dc3f65eb|2024-01-29 14:15:22|\n",
      "|A002|R001|02-00-00|Test-Station1|456NQR  |BMT     |01-29-24|14:15:32|REGULAR|10     |8    |4aa78f73-9793-453d-8608-a5a66394a7dd|2024-01-29 14:15:32|\n",
      "|A002|R002|02-00-00|Test-Station2|456NQR  |BMT     |01-29-24|14:15:42|REGULAR|5      |10   |7c7d244e-676a-4a5f-9a3a-153cccedc193|2024-01-29 14:15:42|\n",
      "|A001|R002|02-00-00|Test-Station1|456NQR  |BMT     |01-29-24|14:15:52|REGULAR|6      |8    |b2f4f413-9e50-41af-a37a-033e94e83425|2024-01-29 14:15:52|\n",
      "+----+----+--------+-------------+--------+--------+--------+--------+-------+-------+-----+------------------------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# convert the schema to string\n",
    "schema_string = turnstiles_schema.simpleString()\n",
    "df_messages = parse_messages(stream, schema=turnstiles_schema)\n",
    "write_to_console(df_messages)\n",
    "# write_to_storage(df_messages)\n",
    "\n",
    "# query = df_messages.writeStream \\\n",
    "#                    .foreachBatch(lambda batch, id: process_batch(batch, id, 'by_message')) \\\n",
    "#                    .start()\n",
    "\n",
    "window_duration = '5 minutes'\n",
    "window_slide = '5 minutes'\n",
    "\n",
    "df_windowed = agg_messages(df_messages,window_duration, window_slide)\n",
    "\n",
    "# Start the streaming query\n",
    "# query = df_windowed.writeStream.outputMode(\"append\").format(\"memory\").queryName(\"output\").start()\n",
    "\n",
    "\n",
    "# query = df_windowed.writeStream \\\n",
    "#                    .foreachBatch(lambda batch, id: process_batch(batch, id, 'by_station')) \\\n",
    "#                    .start()\n",
    "\n",
    "write_to_storage(df=df_windowed, processing_time=window_duration)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping c3659f60-3c2f-4dd2-9eb2-0f105d651232 None\n",
      "Stopping 374c432d-957f-434f-932d-0421f2cb8368 None\n"
     ]
    }
   ],
   "source": [
    "# clean-up session \n",
    "\n",
    "# Stop any active streaming queries (if applicable)\n",
    "active_queries = spark.streams.active\n",
    "if (active_queries) :    \n",
    "    for query in active_queries:\n",
    "        print(f'Stopping {query.id} {query.name}')\n",
    "        query.stop()\n",
    "else:\n",
    "    print(\"No active queries\")\n",
    "\n",
    "\n",
    "# Stop existing SparkSession\n",
    "# spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "# Create a new SparkSession to cleanup folders\n",
    "# spark = SparkSession.builder.appName(\"cleanup\").getOrCreate()\n",
    "# spark.sql(\"DROP TABLE IF EXISTS your_checkpoint_table\")  # Drop the checkpoint table if using structured streaming\n",
    "# spark.stop()\n",
    "\n",
    "\n",
    "def remove_dir(path):\n",
    "    if os.path.exists(path):\n",
    "        shutil.rmtree(path)\n",
    "        print(f'Path {path} removed successfully.')\n",
    "    else:\n",
    "        print(f'Path {path} is not found')\n",
    "\n",
    "try: \n",
    "\n",
    "    # Delete checkpoint directory\n",
    "    checkpoint_dir = \"./checkpoint/\"\n",
    "    remove_dir(checkpoint_dir)\n",
    "\n",
    "    # Delete storage directory\n",
    "    storage_dir = \"./storage/\"\n",
    "    remove_dir(storage_dir)\n",
    "except Exception as ex:\n",
    "     print(f\"Error found {ex}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
