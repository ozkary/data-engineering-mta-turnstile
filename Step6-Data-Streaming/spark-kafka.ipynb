{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Kafka Data Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "from pyspark.sql.functions import udf, col, from_json, from_csv, sum as _sum, unix_timestamp\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType, DateType\n",
    "from pyspark.sql import DataFrame\n",
    "import datetime\n",
    "\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.1,org.apache.spark:spark-avro_2.12:3.3.1 pyspark-shell'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Configuration file reader\n",
    "\n",
    "def read_config(config_file):\n",
    "    \"\"\"\n",
    "    Reads the kafka configuration information that is stored in the system    \n",
    "    \"\"\"\n",
    "    conf = {}    \n",
    "    with open(config_file) as fh:\n",
    "        for line in fh:\n",
    "            line = line.strip()\n",
    "            if len(line) != 0 and line[0] != \"#\":\n",
    "                parameter, value = line.strip().split('=', 1)\n",
    "                conf[parameter] = value.strip()          \n",
    "    return conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the local configuration files\n",
    "\n",
    "config_path = os.path.join(os.path.dirname('/home/ozkary/.kafka/'),'localhost-nosasl.properties')\n",
    "config = read_config(config_path)\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, DataFrame\n",
    "import pyspark.sql.types as T\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Spark-Notebook\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read from Kafka Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "topic = 'mta-turnstile'\n",
    "client_id = 'Spark-Notebook-Session'\n",
    "group_id = 'turnstile'\n",
    "\n",
    "use_sasl = \"sasl.mechanism\" in config and config[\"sasl.mechanism\"] is not None\n",
    "\n",
    "kafka_options = {\n",
    "            \"kafka.bootstrap.servers\": config[\"bootstrap.servers\"],\n",
    "            \"subscribe\": topic,\n",
    "            \"startingOffsets\": \"latest\",\n",
    "            \"failOnDataLoss\": \"false\",\n",
    "            \"client.id\": client_id,            \n",
    "            \"group.id\": group_id,            \n",
    "            \"auto.offset.reset\": \"latest\",\n",
    "            \"checkpointLocation\": \"checkpoint\",\n",
    "            \"minPartitions\": \"2\",\n",
    "            \"enable.auto.commit\": \"false\",\n",
    "            \"enable.partition.eof\": \"true\"                        \n",
    "        }          \n",
    "\n",
    "if use_sasl:\n",
    "    # set the JAAS configuration only when use_sasl is True\n",
    "    sasl_config = f'org.apache.kafka.common.security.plain.PlainLoginModule required serviceName=\"kafka\" username=\"{self.settings[\"sasl.username\"]}\" password=\"{self.settings[\"sasl.password\"]}\";'\n",
    "\n",
    "    login_options = {\n",
    "        \"kafka.sasl.mechanisms\": self.settings[\"sasl.mechanism\"],\n",
    "        \"kafka.security.protocol\": self.settings[\"security.protocol\"],\n",
    "        \"kafka.sasl.username\": self.settings[\"sasl.username\"],\n",
    "        \"kafka.sasl.password\": self.settings[\"sasl.password\"],  \n",
    "        \"kafka.sasl.jaas.config\": sasl_config          \n",
    "    }\n",
    "    # merge the login options with the kafka options\n",
    "    kafka_options = {**kafka_options, **login_options}  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_deserializer(value: bytes) -> any:\n",
    "    \"\"\"\n",
    "    Message value deserializer\n",
    "    \"\"\"\n",
    "    return json.loads(value) \n",
    "\n",
    "# set the stream source\n",
    "# default for startingOffsets is \"latest\"\n",
    "stream = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .options(**kafka_options) \\\n",
    "    .option(\"key.deserializer\", value_deserializer) \\\n",
    "    .option(\"value.deserializer\", value_deserializer) \\\n",
    "    .load()\n",
    "\n",
    "\n",
    "stream.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def write_to_console(df: DataFrame, output_mode: str = 'append', processing_time: str = '60 seconds') -> None:\n",
    "    \"\"\"\n",
    "        Output stream values to the console\n",
    "    \"\"\"\n",
    "    \n",
    "    console_query = df.writeStream\\\n",
    "        .outputMode(output_mode) \\\n",
    "        .trigger(processingTime=processing_time) \\\n",
    "        .format(\"console\") \\\n",
    "        .option(\"truncate\", False) \\\n",
    "        .start()\n",
    "    \n",
    "    # console_query.awaitTermination()   \n",
    "\n",
    "# write a streaming data frame to storage ./storage\n",
    "def write_to_storage(df: DataFrame, output_mode: str = 'append', processing_time: str = '60 seconds') -> None:\n",
    "    \"\"\"\n",
    "        Output stream values to the console\n",
    "    \"\"\"\n",
    "\n",
    "    # if \"window.start\" in df.columns and \"window.end\" in df.columns:\n",
    "    #     df_csv = df.select(\n",
    "    #         col(\"window.start\").alias(\"START_DT\"),\n",
    "    #         col(\"window.end\").alias(\"END_DT\"),\n",
    "    #         \"A/C\", \"UNIT\", \"SCP\", \"STATION\", \"LINENAME\", \"DIVISION\", \"DATE\", \"DESC\",\n",
    "    #         \"ENTRIES\", \"EXITS\"\n",
    "    #     )\n",
    "    # else:\n",
    "    df_csv = df.select(\n",
    "        \"A/C\", \"UNIT\", \"SCP\", \"STATION\", \"LINENAME\", \"DIVISION\", \"DATE\", \"DESC\",\n",
    "        \"ENTRIES\", \"EXITS\"\n",
    "    )\n",
    "        \n",
    "    # .partitionBy(\"STATION\") \\\n",
    "    storage_query = df_csv.writeStream \\\n",
    "        .outputMode(output_mode) \\\n",
    "        .trigger(processingTime=processing_time) \\\n",
    "        .format(\"csv\") \\\n",
    "        .option(\"header\", True) \\\n",
    "        .option(\"path\", \"./storage\") \\\n",
    "        .option(\"checkpointLocation\", \"./checkpoint\") \\\n",
    "        .option(\"truncate\", False) \\\n",
    "        .start()\n",
    "    \n",
    "    # storage_query.awaitTermination()\n",
    "\n",
    "# Define the schema for the incoming data\n",
    "turnstiles_schema = StructType([\n",
    "    StructField(\"`A/C`\", StringType()),\n",
    "    StructField(\"UNIT\", StringType()),\n",
    "    StructField(\"SCP\", StringType()),\n",
    "    StructField(\"STATION\", StringType()),\n",
    "    StructField(\"LINENAME\", StringType()),\n",
    "    StructField(\"DIVISION\", StringType()),\n",
    "    StructField(\"DATE\", StringType()),\n",
    "    StructField(\"TIME\", StringType()),\n",
    "    StructField(\"DESC\", StringType()),\n",
    "    StructField(\"ENTRIES\", IntegerType()),\n",
    "    StructField(\"EXITS\", IntegerType()),\n",
    "    StructField(\"ID\", StringType()),\n",
    "    StructField(\"TIMESTAMP\", StringType())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "dt_ts = datetime.now()\n",
    "format = \"%Y-%m-%d %H:%M:%S\"  \n",
    "ts2 = dt_ts.strftime(format)\n",
    "ts = dt_ts.timestamp()\n",
    "print(dt_ts, ts, ts2)\n",
    "\n",
    "# timestamp = F.to_timestamp(ts2, \"yyyy-MM-dd HH:mm:ss\")\n",
    "\n",
    "# Create a DataFrame with a single row and column\n",
    "try:\n",
    "    df_ts = spark.createDataFrame([(ts2,)], [\"timestamp_str\"])\n",
    "    df_ts = df_ts.withColumn(\"timestamp\", F.to_timestamp(\"timestamp_str\", \"yyyy-MM-dd HH:mm:ss\"))\n",
    "    timestamp_value = df_ts.select(\"timestamp\").first()[0]  # Get the first (and only) value\n",
    "    print(\"Timestamp:\", timestamp_value)\n",
    "except Exception as e:\n",
    "    print(\"Error during timestamp conversion:\", e)\n",
    "# cast variables ts and ts2 to TimestampType\n",
    "# ts_1 = datetime.fromtimestamp(ts)\n",
    "# ts2_1 = datetime.strptime(ts2, \"%Y-%m-%d-%H-%M-%S\")\n",
    "# print(ts_1, ts2_1)\n",
    "\n",
    "test_schema = StructType([  \n",
    "    StructField(\"ID\", StringType()),\n",
    "    StructField(\"TIMESTAMP\", StringType())\n",
    "])\n",
    "\n",
    "# csv = ['\"57da26ec-1a28-4f4b-9966-af160c7a086b\",1704992147','\"0cfe31f5-3132-4190-85d6-a77a032b2af1\",1704992087']\n",
    "# csv = ['57da26ec-1a28-4f4b-9966-af160c7a086b,2024-01-11 11:55:47.37351','0cfe31f5-3132-4190-85d6-a77a032b2af1,2024-01-11 11:55:07.295672']\n",
    "csv = '57da26ec-1a28-4f4b-9966-af160c7a086b,2024-01-11 11:55:47.37351'\n",
    "\n",
    "\n",
    "# get a spark context and use that to load an rdd from the csv string\n",
    "# sc = spark.sparkContext\n",
    "# rdd = sc.parallelize([csv])\n",
    "\n",
    "# # create a dataframe from the rdd and the schema\n",
    "# df_csv = spark.createDataFrame(rdd, test_schema)\n",
    "# df_csv.printSchema()\n",
    "\n",
    "# # print the dataframe\n",
    "# df_csv.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parse_messages(stream, schema) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Parse the messages and use the provided schema to type cast the fields\n",
    "    \"\"\"\n",
    "    assert stream.isStreaming is True, \"DataFrame doesn't receive streaming data\"\n",
    "\n",
    "    options =  {'header': 'true', 'sep': ','}\n",
    "    df = stream.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\", \"timestamp\")               \n",
    "                                \n",
    "    # print(\"df =====>\",df)\n",
    "    # split attributes to nested array in one Column\n",
    "    col = F.split(df['value'], ',')\n",
    "    \n",
    "    # expand col to multiple top-level columns\n",
    "    for idx, field in enumerate(schema):\n",
    "        df = df.withColumn(field.name.replace('`',''), col.getItem(idx).cast(field.dataType))\n",
    "\n",
    "    # Explicitly cast TIMESTAMP column\n",
    "    # df = df.withColumn(\"TIMESTAMP\", F.col(\"TIMESTAMP\").cast(\"timestamp\"))\n",
    "        \n",
    "    # remove quotes from TIMESTAMP column\n",
    "    df = df.withColumn(\"TIMESTAMP\", F.regexp_replace(F.col(\"TIMESTAMP\"), '\"', ''))\n",
    "    \n",
    "    result = df.select([field.name for field in schema])    \n",
    "\n",
    "    df.dropDuplicates([\"ID\",\"STATION\",\"TIMESTAMP\"])\n",
    "\n",
    "    result.printSchema()\n",
    "    \n",
    "    return result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agg_messages(df, window_duration: str, window_slide: str) -> DataFrame:\n",
    "        \"\"\"\n",
    "            Window for n minutes aggregations group by A/C, UNIT, STATION, DATE, DESC\n",
    "        \"\"\"\n",
    "        # df = df.na.fill(0)\n",
    "\n",
    "        # Filter out empty rows\n",
    "        # df = df.filter(col(\"A/C\").isNotNull() & col(\"UNIT\").isNotNull() & col(\"STATION\").isNotNull())\n",
    "\n",
    "        # .withWatermark(\"TIMESTAMP\", window_duration) \\\n",
    "        # .groupBy(F.window(\"TIMESTAMP\", window_duration, window_slide),\"A/C\", \"UNIT\",\"SCP\",\"LINENAME\",\"DIVISION\", \"STATION\", \"DATE\", \"DESC\") \\\n",
    "\n",
    "        df_windowed = df \\\n",
    "            .groupBy(F.window(\"TIMESTAMP\", window_duration, window_slide),\"A/C\", \"UNIT\",\"SCP\",\"LINENAME\",\"DIVISION\", \"STATION\", \"DATE\", \"DESC\") \\\n",
    "            .agg(\n",
    "                _sum(\"ENTRIES\").alias(\"ENTRIES\"),\n",
    "                _sum(\"EXITS\").alias(\"EXITS\")\n",
    "            )    \n",
    "        \n",
    "        # df_windowed.printSchema()    \n",
    "        print(\"df_windowed =====>\",df_windowed)        \n",
    "        # df_windowed.show(10, False)\n",
    "        \n",
    "\n",
    "        return df_windowed\n",
    "\n",
    "# Define a UDF to convert the string timestamp to numeric value\n",
    "def to_timestamp_numeric(timestamp_str):\n",
    "    timestamp = F.to_timestamp(timestamp_str, \"yyyy-MM-dd HH:mm:ss\")\n",
    "    return timestamp.cast(\"long\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_by_station(df, window_duration: str, window_slide: str) -> DataFrame:\n",
    "    \n",
    "    # Ensure TIMESTAMP is in the correct format (timestamp type)        \n",
    "    # df = df.withColumn(\"TIMESTAMP\", F.col(\"TIMESTAMP\").cast(\"timestamp\"))\n",
    "    date_format = \"yyyy-MM-dd HH:mm:ss\"\n",
    "    # df = df.withColumn(\"TIMESTAMP\", unix_timestamp(\"TIMESTAMP\", date_format))\n",
    "    # df = df.withColumn(\"TS\", F.unix_timestamp(\"TIMESTAMP\", date_format))\n",
    "        \n",
    "    df = df.withColumn(\"TS\", F.to_timestamp(\"TIMESTAMP\", date_format))    \n",
    "\n",
    "    df_windowed = df \\\n",
    "        .withWatermark(\"TS\", window_duration) \\\n",
    "        .groupBy(F.window(\"TS\", window_duration), \"STATION\") \\\n",
    "        .agg(\n",
    "            F.sum(\"ENTRIES\").alias(\"ENTRIES\"),\n",
    "            F.sum(\"EXITS\").alias(\"EXITS\")\n",
    "        ).withColumn(\"START\",F.col(\"window.start\")) \\\n",
    "        .withColumn(\"END\", F.col(\"window.end\")) \\\n",
    "        .drop(\"window\") \\\n",
    "        .select(\"STATION\",\"START\",\"END\",\"ENTRIES\",\"EXITS\")\n",
    "    \n",
    "    df_windowed.printSchema()\n",
    "    return df_windowed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch(df, id, tag='message'):\n",
    "\n",
    "    # get the values from the first row\n",
    "    row = df.first()\n",
    "    # check if the TIMESTAMP value can be casted as timestamp\n",
    "    # if not, the row is invalid and we can skip the batch\n",
    "\n",
    "    # if row is None:\n",
    "    #     # print(f\"Invalid {tag} batch {id}\")\n",
    "    #     return\n",
    "    \n",
    "    # ts = row['TIMESTAMP']\n",
    "\n",
    "    # try:\n",
    "    #     row['TIMESTAMP'].cast(\"timestamp\")\n",
    "    # except:\n",
    "    #     print(f\"Invalid TIMESTAMP {ts} value in batch {id}\")\n",
    "    \n",
    "    print(f\"Processing {tag} batch {id} with {df.count()} records. {row}\")\n",
    "    # if df.isEmpty():\n",
    "    #     print(f\"DataFrame is empty in this batch {id}.\")\n",
    "    #     # Handle empty DataFrame as needed\n",
    "    # else:\n",
    "    #      print(\"Data found in this batch.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4070:=============================>                       (55 + 8) / 100]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# convert the schema to string\n",
    "schema_string = turnstiles_schema.simpleString()\n",
    "df_messages = parse_messages(stream, schema=turnstiles_schema)\n",
    "write_to_console(df_messages)\n",
    "# write_to_storage(df_messages)\n",
    "\n",
    "# query = df_messages.writeStream \\\n",
    "#                    .foreachBatch(lambda batch, id: process_batch(batch, id, 'by_message')) \\\n",
    "#                    .start()\n",
    "\n",
    "window_duration = '2 minutes'\n",
    "window_slide = '1 minutes'\n",
    "\n",
    "df_windowed = add_by_station(df_messages,window_duration, window_slide)\n",
    "write_to_console(df_windowed)\n",
    "\n",
    "# query = df_windowed.writeStream \\\n",
    "#                    .foreachBatch(lambda batch, id: process_batch(batch, id, 'by_station')) \\\n",
    "#                    .start()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
