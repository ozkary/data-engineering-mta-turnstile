{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Kafka Data Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "from pyspark.sql.functions import udf, col, from_json, from_csv, sum as _sum\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType, DateType\n",
    "\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.1,org.apache.spark:spark-avro_2.12:3.3.1 pyspark-shell'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Configuration file reader\n",
    "\n",
    "def read_config(config_file):\n",
    "    \"\"\"\n",
    "    Reads the kafka configuration information that is stored in the system    \n",
    "    \"\"\"\n",
    "    conf = {}    \n",
    "    with open(config_file) as fh:\n",
    "        for line in fh:\n",
    "            line = line.strip()\n",
    "            if len(line) != 0 and line[0] != \"#\":\n",
    "                parameter, value = line.strip().split('=', 1)\n",
    "                conf[parameter] = value.strip()          \n",
    "    return conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the local configuration files\n",
    "\n",
    "config_path = os.path.join(os.path.dirname('/home/ozkary/.kafka/'),'localhost-nosasl.properties')\n",
    "config = read_config(config_path)\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.types as T\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Spark-Notebook\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read from Kafka Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "topic = 'mta-turnstile'\n",
    "client_id = 'Spark-Notebook-Session'\n",
    "group_id = 'turnstile'\n",
    "\n",
    "use_sasl = \"sasl.mechanism\" in config and config[\"sasl.mechanism\"] is not None\n",
    "\n",
    "kafka_options = {\n",
    "            \"kafka.bootstrap.servers\": config[\"bootstrap.servers\"],\n",
    "            \"subscribe\": topic,\n",
    "            \"startingOffsets\": \"latest\",\n",
    "            \"failOnDataLoss\": \"false\",\n",
    "            \"client.id\": client_id,            \n",
    "            \"group.id\": group_id,            \n",
    "            \"auto.offset.reset\": \"latest\",\n",
    "            \"checkpointLocation\": \"checkpoint\",\n",
    "            \"minPartitions\": \"2\",\n",
    "            \"enable.auto.commit\": \"false\",\n",
    "            \"enable.partition.eof\": \"true\"                        \n",
    "        }          \n",
    "\n",
    "if use_sasl:\n",
    "    # set the JAAS configuration only when use_sasl is True\n",
    "    sasl_config = f'org.apache.kafka.common.security.plain.PlainLoginModule required serviceName=\"kafka\" username=\"{self.settings[\"sasl.username\"]}\" password=\"{self.settings[\"sasl.password\"]}\";'\n",
    "\n",
    "    login_options = {\n",
    "        \"kafka.sasl.mechanisms\": self.settings[\"sasl.mechanism\"],\n",
    "        \"kafka.security.protocol\": self.settings[\"security.protocol\"],\n",
    "        \"kafka.sasl.username\": self.settings[\"sasl.username\"],\n",
    "        \"kafka.sasl.password\": self.settings[\"sasl.password\"],  \n",
    "        \"kafka.sasl.jaas.config\": sasl_config          \n",
    "    }\n",
    "    # merge the login options with the kafka options\n",
    "    kafka_options = {**kafka_options, **login_options}  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_deserializer(value: bytes) -> any:\n",
    "    \"\"\"\n",
    "    Message value deserializer\n",
    "    \"\"\"\n",
    "    return json.loads(value) \n",
    "\n",
    "# set the stream source\n",
    "# default for startingOffsets is \"latest\"\n",
    "stream = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .options(**kafka_options) \\\n",
    "    .option(\"key.deserializer\", value_deserializer) \\\n",
    "    .option(\"value.deserializer\", value_deserializer) \\\n",
    "    .load()\n",
    "\n",
    "\n",
    "stream.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def write_to_console(df: pd.DataFrame, output_mode: str = 'append', processing_time: str = '15 seconds') -> None:\n",
    "    \"\"\"\n",
    "        Output stream values to the console\n",
    "    \"\"\"\n",
    "    \n",
    "    console_query = df.writeStream\\\n",
    "        .outputMode(output_mode) \\\n",
    "        .trigger(processingTime=processing_time) \\\n",
    "        .format(\"console\") \\\n",
    "        .option(\"truncate\", False) \\\n",
    "        .start()\n",
    "    \n",
    "    console_query.awaitTermination()   \n",
    "\n",
    "# write a streaming data frame to storage ./storage\n",
    "def write_to_storage(df: pd.DataFrame, output_mode: str = 'append', processing_time: str = '15 seconds') -> None:\n",
    "    \"\"\"\n",
    "        Output stream values to the console\n",
    "    \"\"\"\n",
    "\n",
    "    # if \"window.start\" in df.columns and \"window.end\" in df.columns:\n",
    "    #     df_csv = df.select(\n",
    "    #         col(\"window.start\").alias(\"START_DT\"),\n",
    "    #         col(\"window.end\").alias(\"END_DT\"),\n",
    "    #         \"A/C\", \"UNIT\", \"SCP\", \"STATION\", \"LINENAME\", \"DIVISION\", \"DATE\", \"DESC\",\n",
    "    #         \"ENTRIES\", \"EXITS\"\n",
    "    #     )\n",
    "    # else:\n",
    "    df_csv = df.select(\n",
    "        \"A/C\", \"UNIT\", \"SCP\", \"STATION\", \"LINENAME\", \"DIVISION\", \"DATE\", \"DESC\",\n",
    "        \"ENTRIES\", \"EXITS\"\n",
    "    )\n",
    "        \n",
    "    # .partitionBy(\"STATION\") \\\n",
    "    storage_query = df_csv.writeStream \\\n",
    "        .outputMode(output_mode) \\\n",
    "        .trigger(processingTime=processing_time) \\\n",
    "        .format(\"csv\") \\\n",
    "        .option(\"header\", True) \\\n",
    "        .option(\"path\", \"./storage\") \\\n",
    "        .option(\"checkpointLocation\", \"./checkpoint\") \\\n",
    "        .option(\"truncate\", False) \\\n",
    "        .start()\n",
    "    \n",
    "    storage_query.awaitTermination()\n",
    "\n",
    "# Define the schema for the incoming data\n",
    "turnstiles_schema = StructType([\n",
    "    StructField(\"`A/C`\", StringType()),\n",
    "    StructField(\"UNIT\", StringType()),\n",
    "    StructField(\"SCP\", StringType()),\n",
    "    StructField(\"STATION\", StringType()),\n",
    "    StructField(\"LINENAME\", StringType()),\n",
    "    StructField(\"DIVISION\", StringType()),\n",
    "    StructField(\"DATE\", StringType()),\n",
    "    StructField(\"TIME\", StringType()),\n",
    "    StructField(\"DESC\", StringType()),\n",
    "    StructField(\"ENTRIES\", IntegerType()),\n",
    "    StructField(\"EXITS\", IntegerType()),\n",
    "    StructField(\"ID\", StringType()),\n",
    "    StructField(\"TIMESTAMP\", TimestampType())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parse_messages(stream, schema) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Parse the messages and use the provided schema to type cast the fields\n",
    "    \"\"\"\n",
    "    assert stream.isStreaming is True, \"DataFrame doesn't receive streaming data\"\n",
    "\n",
    "    options =  {'header': 'true', 'sep': ','}\n",
    "    df = stream.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")               \n",
    "                                \n",
    "    # print(\"df =====>\",df)\n",
    "    # split attributes to nested array in one Column\n",
    "    col = F.split(df['value'], ',')\n",
    "\n",
    "    # expand col to multiple top-level columns\n",
    "    for idx, field in enumerate(schema):\n",
    "        df = df.withColumn(field.name.replace('`',''), col.getItem(idx).cast(field.dataType))\n",
    "\n",
    "    result = df.select([field.name for field in schema])    \n",
    "    result.printSchema()\n",
    "    \n",
    "    return result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agg_messages(df, window_duration: str, window_slide: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "            Window for n minutes aggregations group by A/C, UNIT, STATION, DATE, DESC\n",
    "        \"\"\"\n",
    "        # df = df.na.fill(0)\n",
    "\n",
    "        # Filter out empty rows\n",
    "        # df = df.filter(col(\"A/C\").isNotNull() & col(\"UNIT\").isNotNull() & col(\"STATION\").isNotNull())\n",
    "\n",
    "        # .withWatermark(\"TIMESTAMP\", window_duration) \\\n",
    "        # .groupBy(F.window(\"TIMESTAMP\", window_duration, window_slide),\"A/C\", \"UNIT\",\"SCP\",\"LINENAME\",\"DIVISION\", \"STATION\", \"DATE\", \"DESC\") \\\n",
    "\n",
    "        df_windowed = df \\\n",
    "            .groupBy(F.window(\"TIMESTAMP\", window_duration, window_slide),\"A/C\", \"UNIT\",\"SCP\",\"LINENAME\",\"DIVISION\", \"STATION\", \"DATE\", \"DESC\") \\\n",
    "            .agg(\n",
    "                _sum(\"ENTRIES\").alias(\"ENTRIES\"),\n",
    "                _sum(\"EXITS\").alias(\"EXITS\")\n",
    "            )    \n",
    "        \n",
    "        # df_windowed.printSchema()    \n",
    "        print(\"df_windowed =====>\",df_windowed)        \n",
    "        # df_windowed.show(10, False)\n",
    "\n",
    "        return df_windowed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_by_station(df, window_duration: str, window_slide: str) -> pd.DataFrame:\n",
    "    \n",
    "      # Ensure TIMESTAMP is in the correct format (timestamp type)\n",
    "    df = df.withColumn(\"TIMESTAMP\", F.col(\"TIMESTAMP\").cast(\"timestamp\"))\n",
    "    \n",
    "    # Group by 'STATION' and create a 2-minute window\n",
    "    df_windowed = df.groupBy(df[\"STATION\"], F.window(df[\"TIMESTAMP\"], window_duration)).agg(\n",
    "        F.sum(df[\"ENTRIES\"]).alias(\"ENTRIES\"),\n",
    "        F.sum(df[\"EXITS\"]).alias(\"EXITS\")\n",
    "    ).withColumn(\"START\", F.col(\"window.start\")).withColumn(\"END\", F.col(\"window.end\"))\n",
    "\n",
    "    # Drop the original window column\n",
    "    # df_windowed.drop(\"window\")\n",
    "\n",
    "    df_windowed.printSchema()\n",
    "    return df_windowed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- A/C: string (nullable = true)\n",
      " |-- UNIT: string (nullable = true)\n",
      " |-- SCP: string (nullable = true)\n",
      " |-- STATION: string (nullable = true)\n",
      " |-- LINENAME: string (nullable = true)\n",
      " |-- DIVISION: string (nullable = true)\n",
      " |-- DATE: string (nullable = true)\n",
      " |-- TIME: string (nullable = true)\n",
      " |-- DESC: string (nullable = true)\n",
      " |-- ENTRIES: integer (nullable = true)\n",
      " |-- EXITS: integer (nullable = true)\n",
      " |-- ID: string (nullable = true)\n",
      " |-- TIMESTAMP: timestamp (nullable = true)\n",
      "\n",
      "23/11/17 14:06:12 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-3e4cfc04-2522-46ba-8ef2-93577416fdd8. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "23/11/17 14:06:12 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+---+----+---+-------+--------+--------+----+----+----+-------+-----+---+---------+\n",
      "|A/C|UNIT|SCP|STATION|LINENAME|DIVISION|DATE|TIME|DESC|ENTRIES|EXITS|ID |TIMESTAMP|\n",
      "+---+----+---+-------+--------+--------+----+----+----+-------+-----+---+---------+\n",
      "+---+----+---+-------+--------+--------+----+----+----+-------+-----+---+---------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 8\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "Batch: 81\n",
      "-------------------------------------------\n",
      "+-----+----+--------+------------+--------+--------+--------+--------+-------+-------+-----+------------------------------------+---------+\n",
      "|A/C  |UNIT|SCP     |STATION     |LINENAME|DIVISION|DATE    |TIME    |DESC   |ENTRIES|EXITS|ID                                  |TIMESTAMP|\n",
      "+-----+----+--------+------------+--------+--------+--------+--------+-------+-------+-----+------------------------------------+---------+\n",
      "|\"A001|R002|02-00-00|Test-Station|456NQR  |BMT     |11-17-23|14:06:07|REGULAR|675    |630  |61ea8d9f-643c-4b6b-a858-a8d2f8aa8e06|null     |\n",
      "+-----+----+--------+------------+--------+--------+--------+--------+-------+-------+-----+------------------------------------+---------+\n",
      "\n",
      "+-----+----+--------+------------+--------+--------+--------+--------+-------+-------+-----+------------------------------------+---------+\n",
      "|A/C  |UNIT|SCP     |STATION     |LINENAME|DIVISION|DATE    |TIME    |DESC   |ENTRIES|EXITS|ID                                  |TIMESTAMP|\n",
      "+-----+----+--------+------------+--------+--------+--------+--------+-------+-------+-----+------------------------------------+---------+\n",
      "|\"A001|R002|02-00-00|Test-Station|456NQR  |BMT     |11-17-23|14:06:07|REGULAR|675    |630  |61ea8d9f-643c-4b6b-a858-a8d2f8aa8e06|null     |\n",
      "+-----+----+--------+------------+--------+--------+--------+--------+-------+-------+-----+------------------------------------+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 9\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "Batch: 82\n",
      "-------------------------------------------\n",
      "+-----+----+--------+------------+--------+--------+--------+--------+-------+-------+-----+------------------------------------+---------+\n",
      "|A/C  |UNIT|SCP     |STATION     |LINENAME|DIVISION|DATE    |TIME    |DESC   |ENTRIES|EXITS|ID                                  |TIMESTAMP|\n",
      "+-----+----+--------+------------+--------+--------+--------+--------+-------+-------+-----+------------------------------------+---------+\n",
      "|\"A002|R002|02-00-00|Test-Station|456NQR  |BMT     |11-17-23|14:06:17|REGULAR|772    |620  |7935d138-a53b-4aa2-ad70-9824b6c4f14c|null     |\n",
      "|\"A002|R001|02-00-00|Test-Station|456NQR  |BMT     |11-17-23|14:06:27|REGULAR|534    |816  |bb495fde-597e-4cc5-b986-fc2b0fbb45e6|null     |\n",
      "+-----+----+--------+------------+--------+--------+--------+--------+-------+-------+-----+------------------------------------+---------+\n",
      "\n",
      "+-----+----+--------+------------+--------+--------+--------+--------+-------+-------+-----+------------------------------------+---------+\n",
      "|A/C  |UNIT|SCP     |STATION     |LINENAME|DIVISION|DATE    |TIME    |DESC   |ENTRIES|EXITS|ID                                  |TIMESTAMP|\n",
      "+-----+----+--------+------------+--------+--------+--------+--------+-------+-------+-----+------------------------------------+---------+\n",
      "|\"A002|R002|02-00-00|Test-Station|456NQR  |BMT     |11-17-23|14:06:17|REGULAR|772    |620  |7935d138-a53b-4aa2-ad70-9824b6c4f14c|null     |\n",
      "|\"A002|R001|02-00-00|Test-Station|456NQR  |BMT     |11-17-23|14:06:27|REGULAR|534    |816  |bb495fde-597e-4cc5-b986-fc2b0fbb45e6|null     |\n",
      "+-----+----+--------+------------+--------+--------+--------+--------+-------+-------+-----+------------------------------------+---------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+-----+----+--------+------------+--------+--------+--------+--------+-------+-------+-----+------------------------------------+---------+\n",
      "|A/C  |UNIT|SCP     |STATION     |LINENAME|DIVISION|DATE    |TIME    |DESC   |ENTRIES|EXITS|ID                                  |TIMESTAMP|\n",
      "+-----+----+--------+------------+--------+--------+--------+--------+-------+-------+-----+------------------------------------+---------+\n",
      "|\"A002|R002|02-00-00|Test-Station|456NQR  |BMT     |11-17-23|14:06:17|REGULAR|772    |620  |7935d138-a53b-4aa2-ad70-9824b6c4f14c|null     |\n",
      "|\"A002|R001|02-00-00|Test-Station|456NQR  |BMT     |11-17-23|14:06:27|REGULAR|534    |816  |bb495fde-597e-4cc5-b986-fc2b0fbb45e6|null     |\n",
      "+-----+----+--------+------------+--------+--------+--------+--------+-------+-------+-----+------------------------------------+---------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 83\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "Batch: 10\n",
      "-------------------------------------------\n",
      "+-----+----+--------+------------+--------+--------+--------+--------+-------+-------+-----+------------------------------------+---------+\n",
      "|A/C  |UNIT|SCP     |STATION     |LINENAME|DIVISION|DATE    |TIME    |DESC   |ENTRIES|EXITS|ID                                  |TIMESTAMP|\n",
      "+-----+----+--------+------------+--------+--------+--------+--------+-------+-------+-----+------------------------------------+---------+\n",
      "|\"A002|R002|02-00-00|Test-Station|456NQR  |BMT     |11-17-23|14:06:37|REGULAR|501    |690  |f7f9da1c-ac8b-419e-9df2-af29fb7978d2|null     |\n",
      "+-----+----+--------+------------+--------+--------+--------+--------+-------+-------+-----+------------------------------------+---------+\n",
      "\n",
      "+-----+----+--------+------------+--------+--------+--------+--------+-------+-------+-----+------------------------------------+---------+\n",
      "|A/C  |UNIT|SCP     |STATION     |LINENAME|DIVISION|DATE    |TIME    |DESC   |ENTRIES|EXITS|ID                                  |TIMESTAMP|\n",
      "+-----+----+--------+------------+--------+--------+--------+--------+-------+-------+-----+------------------------------------+---------+\n",
      "|\"A002|R002|02-00-00|Test-Station|456NQR  |BMT     |11-17-23|14:06:37|REGULAR|501    |690  |f7f9da1c-ac8b-419e-9df2-af29fb7978d2|null     |\n",
      "+-----+----+--------+------------+--------+--------+--------+--------+-------+-------+-----+------------------------------------+---------+\n",
      "\n",
      "+-----+----+--------+------------+--------+--------+--------+--------+-------+-------+-----+------------------------------------+---------+\n",
      "|A/C  |UNIT|SCP     |STATION     |LINENAME|DIVISION|DATE    |TIME    |DESC   |ENTRIES|EXITS|ID                                  |TIMESTAMP|\n",
      "+-----+----+--------+------------+--------+--------+--------+--------+-------+-------+-----+------------------------------------+---------+\n",
      "|\"A002|R002|02-00-00|Test-Station|456NQR  |BMT     |11-17-23|14:06:37|REGULAR|501    |690  |f7f9da1c-ac8b-419e-9df2-af29fb7978d2|null     |\n",
      "+-----+----+--------+------------+--------+--------+--------+--------+-------+-------+-----+------------------------------------+---------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 84\n",
      "-------------------------------------------\n",
      "+-----+----+--------+------------+--------+--------+--------+--------+-------+-------+-----+------------------------------------+---------+\n",
      "|A/C  |UNIT|SCP     |STATION     |LINENAME|DIVISION|DATE    |TIME    |DESC   |ENTRIES|EXITS|ID                                  |TIMESTAMP|\n",
      "+-----+----+--------+------------+--------+--------+--------+--------+-------+-------+-----+------------------------------------+---------+\n",
      "|\"A001|R002|02-00-00|Test-Station|456NQR  |BMT     |11-17-23|14:06:47|REGULAR|546    |603  |f0f8d98f-1dfe-4023-9494-f47f1a6bf376|null     |\n",
      "|\"A002|R001|02-00-00|Test-Station|456NQR  |BMT     |11-17-23|14:06:57|REGULAR|961    |662  |4dbd61ce-4865-4fb1-a5ad-44711350e92b|null     |\n",
      "+-----+----+--------+------------+--------+--------+--------+--------+-------+-------+-----+------------------------------------+---------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 11\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "Batch: 3\n",
      "-------------------------------------------\n",
      "+-----+----+--------+------------+--------+--------+--------+--------+-------+-------+-----+------------------------------------+---------+\n",
      "|A/C  |UNIT|SCP     |STATION     |LINENAME|DIVISION|DATE    |TIME    |DESC   |ENTRIES|EXITS|ID                                  |TIMESTAMP|\n",
      "+-----+----+--------+------------+--------+--------+--------+--------+-------+-------+-----+------------------------------------+---------+\n",
      "|\"A001|R002|02-00-00|Test-Station|456NQR  |BMT     |11-17-23|14:06:47|REGULAR|546    |603  |f0f8d98f-1dfe-4023-9494-f47f1a6bf376|null     |\n",
      "|\"A002|R001|02-00-00|Test-Station|456NQR  |BMT     |11-17-23|14:06:57|REGULAR|961    |662  |4dbd61ce-4865-4fb1-a5ad-44711350e92b|null     |\n",
      "+-----+----+--------+------------+--------+--------+--------+--------+-------+-------+-----+------------------------------------+---------+\n",
      "\n",
      "+-----+----+--------+------------+--------+--------+--------+--------+-------+-------+-----+------------------------------------+---------+\n",
      "|A/C  |UNIT|SCP     |STATION     |LINENAME|DIVISION|DATE    |TIME    |DESC   |ENTRIES|EXITS|ID                                  |TIMESTAMP|\n",
      "+-----+----+--------+------------+--------+--------+--------+--------+-------+-------+-----+------------------------------------+---------+\n",
      "|\"A001|R002|02-00-00|Test-Station|456NQR  |BMT     |11-17-23|14:06:47|REGULAR|546    |603  |f0f8d98f-1dfe-4023-9494-f47f1a6bf376|null     |\n",
      "|\"A002|R001|02-00-00|Test-Station|456NQR  |BMT     |11-17-23|14:06:57|REGULAR|961    |662  |4dbd61ce-4865-4fb1-a5ad-44711350e92b|null     |\n",
      "+-----+----+--------+------------+--------+--------+--------+--------+-------+-------+-----+------------------------------------+---------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 85\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "Batch: 12\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "Batch: 4\n",
      "-------------------------------------------\n",
      "+-----+----+--------+------------+--------+--------+--------+--------+-------+-------+-----+------------------------------------+---------+\n",
      "|A/C  |UNIT|SCP     |STATION     |LINENAME|DIVISION|DATE    |TIME    |DESC   |ENTRIES|EXITS|ID                                  |TIMESTAMP|\n",
      "+-----+----+--------+------------+--------+--------+--------+--------+-------+-------+-----+------------------------------------+---------+\n",
      "|\"A001|R001|02-00-00|Test-Station|456NQR  |BMT     |11-17-23|14:07:07|REGULAR|520    |800  |65e77807-4e61-4a7c-b106-868f60bf20c6|null     |\n",
      "+-----+----+--------+------------+--------+--------+--------+--------+-------+-------+-----+------------------------------------+---------+\n",
      "\n",
      "+-----+----+--------+------------+--------+--------+--------+--------+-------+-------+-----+------------------------------------+---------+\n",
      "|A/C  |UNIT|SCP     |STATION     |LINENAME|DIVISION|DATE    |TIME    |DESC   |ENTRIES|EXITS|ID                                  |TIMESTAMP|\n",
      "+-----+----+--------+------------+--------+--------+--------+--------+-------+-------+-----+------------------------------------+---------+\n",
      "|\"A001|R001|02-00-00|Test-Station|456NQR  |BMT     |11-17-23|14:07:07|REGULAR|520    |800  |65e77807-4e61-4a7c-b106-868f60bf20c6|null     |\n",
      "+-----+----+--------+------------+--------+--------+--------+--------+-------+-------+-----+------------------------------------+---------+\n",
      "\n",
      "+-----+----+--------+------------+--------+--------+--------+--------+-------+-------+-----+------------------------------------+---------+\n",
      "|A/C  |UNIT|SCP     |STATION     |LINENAME|DIVISION|DATE    |TIME    |DESC   |ENTRIES|EXITS|ID                                  |TIMESTAMP|\n",
      "+-----+----+--------+------------+--------+--------+--------+--------+-------+-------+-----+------------------------------------+---------+\n",
      "|\"A001|R001|02-00-00|Test-Station|456NQR  |BMT     |11-17-23|14:07:07|REGULAR|520    |800  |65e77807-4e61-4a7c-b106-868f60bf20c6|null     |\n",
      "+-----+----+--------+------------+--------+--------+--------+--------+-------+-------+-----+------------------------------------+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ozkary/spark/spark-3.3.2-bin-hadoop3/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/ozkary/spark/spark-3.3.2-bin-hadoop3/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/usr/lib/python3.8/socket.py\", line 669, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/ozkary/workspace/de-mta/Step6-Data-Streaming/spark-kafka.ipynb Cell 13\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/ozkary/workspace/de-mta/Step6-Data-Streaming/spark-kafka.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m schema_string \u001b[39m=\u001b[39m turnstiles_schema\u001b[39m.\u001b[39msimpleString()\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/ozkary/workspace/de-mta/Step6-Data-Streaming/spark-kafka.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m df_messages \u001b[39m=\u001b[39m parse_messages(stream, schema\u001b[39m=\u001b[39mturnstiles_schema)\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/ozkary/workspace/de-mta/Step6-Data-Streaming/spark-kafka.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m write_to_console(df_messages)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/ozkary/workspace/de-mta/Step6-Data-Streaming/spark-kafka.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# write_to_storage(df_messages)\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/ozkary/workspace/de-mta/Step6-Data-Streaming/spark-kafka.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m window_duration \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m2 minutes\u001b[39m\u001b[39m'\u001b[39m\n",
      "\u001b[1;32m/home/ozkary/workspace/de-mta/Step6-Data-Streaming/spark-kafka.ipynb Cell 13\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/ozkary/workspace/de-mta/Step6-Data-Streaming/spark-kafka.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/ozkary/workspace/de-mta/Step6-Data-Streaming/spark-kafka.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m    Output stream values to the console\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/ozkary/workspace/de-mta/Step6-Data-Streaming/spark-kafka.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/ozkary/workspace/de-mta/Step6-Data-Streaming/spark-kafka.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m console_query \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mwriteStream\\\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/ozkary/workspace/de-mta/Step6-Data-Streaming/spark-kafka.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39m.\u001b[39moutputMode(output_mode) \\\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/ozkary/workspace/de-mta/Step6-Data-Streaming/spark-kafka.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39m.\u001b[39mtrigger(processingTime\u001b[39m=\u001b[39mprocessing_time) \\\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/ozkary/workspace/de-mta/Step6-Data-Streaming/spark-kafka.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39m.\u001b[39mformat(\u001b[39m\"\u001b[39m\u001b[39mconsole\u001b[39m\u001b[39m\"\u001b[39m) \\\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/ozkary/workspace/de-mta/Step6-Data-Streaming/spark-kafka.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39m.\u001b[39moption(\u001b[39m\"\u001b[39m\u001b[39mtruncate\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m) \\\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/ozkary/workspace/de-mta/Step6-Data-Streaming/spark-kafka.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39m.\u001b[39mstart()\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/ozkary/workspace/de-mta/Step6-Data-Streaming/spark-kafka.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m console_query\u001b[39m.\u001b[39;49mawaitTermination()\n",
      "File \u001b[0;32m~/spark/spark-3.3.2-bin-hadoop3/python/pyspark/sql/streaming.py:107\u001b[0m, in \u001b[0;36mStreamingQuery.awaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jsq\u001b[39m.\u001b[39mawaitTermination(\u001b[39mint\u001b[39m(timeout \u001b[39m*\u001b[39m \u001b[39m1000\u001b[39m))\n\u001b[1;32m    106\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 107\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jsq\u001b[39m.\u001b[39;49mawaitTermination()\n",
      "File \u001b[0;32m~/spark/spark-3.3.2-bin-hadoop3/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1320\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1313\u001b[0m args_command, temp_args \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_args(\u001b[39m*\u001b[39margs)\n\u001b[1;32m   1315\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client\u001b[39m.\u001b[39;49msend_command(command)\n\u001b[1;32m   1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_id, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname)\n\u001b[1;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m~/spark/spark-3.3.2-bin-hadoop3/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39;49msend_command(command)\n\u001b[1;32m   1039\u001b[0m     \u001b[39mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[39mreturn\u001b[39;00m response, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m~/spark/spark-3.3.2-bin-hadoop3/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[39m=\u001b[39m smart_decode(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstream\u001b[39m.\u001b[39;49mreadline()[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mAnswer received: \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[39m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[39m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.8/socket.py:669\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    667\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    668\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 669\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    670\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    671\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 13\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "Batch: 86\n",
      "-------------------------------------------\n",
      "+-----+----+--------+------------+--------+--------+--------+--------+-------+-------+-----+------------------------------------+---------+\n",
      "|A/C  |UNIT|SCP     |STATION     |LINENAME|DIVISION|DATE    |TIME    |DESC   |ENTRIES|EXITS|ID                                  |TIMESTAMP|\n",
      "+-----+----+--------+------------+--------+--------+--------+--------+-------+-------+-----+------------------------------------+---------+\n",
      "|\"A002|R001|02-00-00|Test-Station|456NQR  |BMT     |11-17-23|14:07:17|REGULAR|945    |700  |a4dddb81-27a7-42b3-9287-0ff2bafae45e|null     |\n",
      "|\"A001|R001|02-00-00|Test-Station|456NQR  |BMT     |11-17-23|14:07:27|REGULAR|768    |902  |a09ad786-0d70-4fbf-9eb9-c13c20ed23ca|null     |\n",
      "+-----+----+--------+------------+--------+--------+--------+--------+-------+-------+-----+------------------------------------+---------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 5\n",
      "-------------------------------------------\n",
      "+-----+----+--------+------------+--------+--------+--------+--------+-------+-------+-----+------------------------------------+---------+\n",
      "|A/C  |UNIT|SCP     |STATION     |LINENAME|DIVISION|DATE    |TIME    |DESC   |ENTRIES|EXITS|ID                                  |TIMESTAMP|\n",
      "+-----+----+--------+------------+--------+--------+--------+--------+-------+-------+-----+------------------------------------+---------+\n",
      "|\"A002|R001|02-00-00|Test-Station|456NQR  |BMT     |11-17-23|14:07:17|REGULAR|945    |700  |a4dddb81-27a7-42b3-9287-0ff2bafae45e|null     |\n",
      "|\"A001|R001|02-00-00|Test-Station|456NQR  |BMT     |11-17-23|14:07:27|REGULAR|768    |902  |a09ad786-0d70-4fbf-9eb9-c13c20ed23ca|null     |\n",
      "+-----+----+--------+------------+--------+--------+--------+--------+-------+-------+-----+------------------------------------+---------+\n",
      "\n",
      "+-----+----+--------+------------+--------+--------+--------+--------+-------+-------+-----+------------------------------------+---------+\n",
      "|A/C  |UNIT|SCP     |STATION     |LINENAME|DIVISION|DATE    |TIME    |DESC   |ENTRIES|EXITS|ID                                  |TIMESTAMP|\n",
      "+-----+----+--------+------------+--------+--------+--------+--------+-------+-------+-----+------------------------------------+---------+\n",
      "|\"A002|R001|02-00-00|Test-Station|456NQR  |BMT     |11-17-23|14:07:17|REGULAR|945    |700  |a4dddb81-27a7-42b3-9287-0ff2bafae45e|null     |\n",
      "|\"A001|R001|02-00-00|Test-Station|456NQR  |BMT     |11-17-23|14:07:27|REGULAR|768    |902  |a09ad786-0d70-4fbf-9eb9-c13c20ed23ca|null     |\n",
      "+-----+----+--------+------------+--------+--------+--------+--------+-------+-------+-----+------------------------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# convert the schema to string\n",
    "schema_string = turnstiles_schema.simpleString()\n",
    "df_messages = parse_messages(stream, schema=turnstiles_schema)\n",
    "write_to_console(df_messages)\n",
    "\n",
    "# write_to_storage(df_messages)\n",
    "\n",
    "window_duration = '2 minutes'\n",
    "window_slide = '2 minutes'\n",
    "\n",
    "df_windowed = agg_messages(df_messages,window_duration, window_slide)\n",
    "    \n",
    "# add a type column to both dataframes to be able to join them\n",
    "# df_messages_with_type = df_messages.withColumn(\"type\", F.lit(\"turnstile\"))\n",
    "# df_windowed_with_type = df_windowed.withColumn(\"type\", F.lit(\"windowed\"))\n",
    "\n",
    "# union the dataframes to produce a single console output\n",
    "# df_union = df_messages_with_type.union(df_windowed_with_type)\n",
    "\n",
    "# write_to_console(df_messages_with_type)\n",
    "\n",
    "# write_to_console(df_windowed)\n",
    "\n",
    "write_to_storage(df_windowed)\n",
    "\n",
    "# wait 3 minutes and then stop the query\n",
    "# time.sleep(180)\n",
    "# console_query.stop()\n",
    "# storage_query.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read all the csv files from storage and show the data\n",
    "df = spark.read.csv('./storage/*.csv', header=True)\n",
    "df.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
